{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(-inf, inf, (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "env=gym.make('LunarLander-v2')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\n",
    "        super(PolicyNetwork,self).__init__()\n",
    "        self.input_dims=input_dims\n",
    "        self.lr=lr\n",
    "        self.fc1_dims=fc1_dims\n",
    "        self.fc2_dims=fc2_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.fc1=nn.Linear(input_dims,fc1_dims)\n",
    "        self.fc2=nn.Linear(fc1_dims,fc2_dims)\n",
    "        self.fc3=nn.Linear(fc2_dims,n_actions)\n",
    "        self.optimizer=optim.Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,observation):\n",
    "        observation=T.Tensor(observation).to(self.device)\n",
    "        x=F.relu(self.fc1(observation))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,lr,input_dims,gamma=0.99,n_actions=4,l1_size=256,l2_size=256):\n",
    "        \n",
    "        self.gamma=gamma\n",
    "        self.reward_memory=[]\n",
    "        self.action_memory=[]\n",
    "        self.policy=PolicyNetwork(lr,input_dims,l1_size,l2_size,n_actions)\n",
    "        \n",
    "    def act(self,observation):\n",
    "        probs=F.softmax(self.policy.forward(observation))\n",
    "        action_probs=T.distributions.Categorical(probs)\n",
    "        action=action_probs.sample()\n",
    "#         print(action)\n",
    "        log_probs=action_probs.log_prob(action)\n",
    "        self.action_memory.append(log_probs)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_rewards(self,reward):\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        self.policy.optimizer.zero_grad()\n",
    "        G=[]\n",
    "        for t in range(len(self.reward_memory)):\n",
    "            G_sum=0\n",
    "            discount=1\n",
    "            \n",
    "            for k in range(t,len(self.reward_memory)):\n",
    "                G_sum+=self.reward_memory[k]*discount\n",
    "                discount*=self.gamma   \n",
    "            G.append(G_sum)\n",
    "            \n",
    "        mean=np.mean(G)\n",
    "        std=np.std(G) if np.std(G)>0 else 1\n",
    "        G=np.array(G)\n",
    "        G=(G-mean)/std\n",
    "        \n",
    "        G=T.tensor(G).to(self.policy.device)\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for g,log_probs in zip(G,self.action_memory):\n",
    "            loss+= -g*log_probs\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        self.policy.optimizer.step()\n",
    "        \n",
    "        \n",
    "        self.action_memory=[]\n",
    "        self.reward_memory=[]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neloy/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 13.324\n",
      "episode 1 score -0.797\n",
      "episode 2 score 242.123\n",
      "episode 3 score 151.705\n",
      "episode 4 score 150.542\n",
      "episode 5 score 278.728\n",
      "episode 6 score 138.147\n",
      "episode 7 score 13.154\n",
      "episode 8 score 261.335\n",
      "episode 9 score 112.215\n",
      "episode 10 score 137.639\n",
      "episode 11 score 132.602\n",
      "episode 12 score 170.373\n",
      "episode 13 score 119.660\n",
      "episode 14 score 136.098\n",
      "episode 15 score 160.104\n",
      "episode 16 score 194.785\n",
      "episode 17 score 127.288\n",
      "episode 18 score 259.441\n",
      "episode 19 score 144.807\n",
      "episode 20 score 184.723\n",
      "episode 21 score 225.352\n",
      "episode 22 score 156.792\n",
      "episode 23 score 202.117\n",
      "episode 24 score 241.180\n",
      "episode 25 score 139.673\n",
      "episode 26 score 150.946\n",
      "episode 27 score 144.497\n",
      "episode 28 score 119.753\n",
      "episode 29 score 155.285\n",
      "episode 30 score -6.979\n",
      "episode 31 score 166.362\n",
      "episode 32 score 122.588\n",
      "episode 33 score 148.438\n",
      "episode 34 score 136.420\n",
      "episode 35 score 55.754\n",
      "episode 36 score 270.505\n",
      "episode 37 score 55.192\n",
      "episode 38 score 133.931\n",
      "episode 39 score 145.088\n",
      "episode 40 score 157.978\n",
      "episode 41 score 132.091\n",
      "episode 42 score 210.822\n",
      "episode 43 score 119.806\n",
      "episode 44 score 243.544\n",
      "episode 45 score -17.852\n",
      "episode 46 score 162.299\n",
      "episode 47 score 188.625\n",
      "episode 48 score 236.407\n",
      "episode 49 score 40.640\n",
      "episode 50 score 163.175\n",
      "episode 51 score 110.508\n",
      "episode 52 score 164.879\n",
      "episode 53 score 222.977\n",
      "episode 54 score 223.466\n",
      "episode 55 score 91.149\n",
      "episode 56 score 156.942\n",
      "episode 57 score 35.810\n",
      "episode 58 score 117.059\n",
      "episode 59 score 127.124\n",
      "episode 60 score 124.754\n",
      "episode 61 score 147.643\n",
      "episode 62 score 170.978\n",
      "episode 63 score 130.725\n",
      "episode 64 score 162.828\n",
      "episode 65 score 85.298\n",
      "episode 66 score 109.641\n",
      "episode 67 score 137.288\n",
      "episode 68 score 176.542\n",
      "episode 69 score 129.730\n",
      "episode 70 score 262.365\n",
      "episode 71 score 127.462\n",
      "episode 72 score 209.726\n",
      "episode 73 score 229.478\n",
      "episode 74 score 218.600\n",
      "episode 75 score 145.315\n",
      "episode 76 score 162.094\n",
      "episode 77 score 131.788\n",
      "episode 78 score 147.145\n",
      "episode 79 score 251.837\n",
      "episode 80 score 144.275\n",
      "episode 81 score 158.593\n",
      "episode 82 score 105.812\n",
      "episode 83 score 97.332\n",
      "episode 84 score 126.698\n",
      "episode 85 score 156.100\n",
      "episode 86 score 146.716\n",
      "episode 87 score 163.953\n",
      "episode 88 score 133.764\n",
      "episode 89 score 129.330\n",
      "episode 90 score 181.117\n",
      "episode 91 score 132.515\n",
      "episode 92 score 212.909\n",
      "episode 93 score 211.123\n",
      "episode 94 score 152.721\n",
      "episode 95 score 125.557\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0d2a90a05c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                            True)\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# agent=Agent(0.0005,8,0.99,4,256,256)\n",
    "# PATH=\"Model_Weights/lunar_agent1.pt\"\n",
    "agent = Agent(0.00025,8,0.99,4,256,256)\n",
    "# agent.policy.load_state_dict(T.load(PATH))\n",
    "# agent.policy.eval()\n",
    "\n",
    "score_history=[]\n",
    "score=0\n",
    "num_ep=2500\n",
    "\n",
    "for i in range(num_ep):\n",
    "    done=False\n",
    "    score=0\n",
    "    observation=env.reset()\n",
    "    while not done:\n",
    "        action=agent.act(observation)\n",
    "        observation,reward,done,_ =env.step(action)\n",
    "        agent.store_rewards(reward)\n",
    "        score+=reward\n",
    "    score_history.append(score)\n",
    "    print('episode',i,'score %.3f' % score)\n",
    "    agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"Model_Weights/lunar_agent1.pt\"\n",
    "# T.save(agent.policy.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (fc1): Linear(in_features=8, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agent = Agent(0.0005,8,0.99,4,256,256)\n",
    "new_agent.policy.load_state_dict(T.load(PATH))\n",
    "new_agent.policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neloy/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "done=False\n",
    "score=0\n",
    "observation=env.reset()\n",
    "while not done:\n",
    "    action=new_agent.act(observation)\n",
    "    observation,reward,done,_ =env.step(action)\n",
    "    env.render()\n",
    "    score+=reward\n",
    "# score_history.append(score)\n",
    "# print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
